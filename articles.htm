
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Article Scraping Demo</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                line-height: 1.5;
            }
            h1 {
                font-size: 36px;
                margin-bottom: 0.5em;
            }
            .article {
                margin-bottom: 3em;
            }
            .article h2 {
                font-size: 24px;
                margin-bottom: 0.5em;
            }
            .article p {
                font-size: 16px;
                margin-bottom: 1em;
            }
        </style>
    </head>
    <body>
    <div class="article">
<h2>Introducing Motherbrain: Big data & machine learning meets private equity</h2>
<p>Oct 11, 2022
Save
The power of artificial intelligence and machine learning is all around us. Unlock your phone using facial recognition, scroll through your personalized social media feed, use a travel app to get to work — all use AI and machine learning. You would also think that these technologies would be central to the investment industry. In some ways you would be right, but in others there is still a long way to go.
Traditionally, financial investors relied on personal networks to find the best opportunities; it was a case of “who you know, not what you know”. In recent decades, investors in public markets have turned to a more data-driven approach that promotes automation, objectiveness and consistency. This has been facilitated by the large amounts of data available on publicly traded companies, which is essential for machine learning platforms. And here is where private capital investors have struggled — there simply isn’t as much data available on private companies.
But as our world becomes increasingly digitized and large volumes of high-quality data on private companies becomes publicly accessible, this is changing. AI and machine learning can be used to sift through millions of data points everyday, at speeds far greater than could be expected of humans.
This is where Motherbrain comes in. Motherbrain leverages big data and machine learning to give EQT, a purpose-driven global investment organization with nearly €80 billion assets under management, a unique edge. Built by a dedicated in-house team of data scientists, machine learning engineers, data engineers, product designers, full stack developers and researchers, the proprietary platform helps EQT make faster, smarter and fairer decisions.
Motherbrain is deeply ingrained into EQT’s processes and used by all investment teams across EQT’s 23 offices globally. It plays three key roles:
We believe that Motherbrain is a pioneer in the private capital industry, but it doesn’t end there — we want to learn from the AI and machine learning community as a whole, while giving back. We’ve already started to do this: for example, two research papers authored by Motherbrain were published by EMNLP (the 2021 Conference on Empirical Methods in Natural Language Processing) and CIKM (the 2022 ACM International Conference on Information and Knowledge Management) respectively. EMNLP and CIKM are acknowledged as the top-tier academic conferences in natural language processing and data mining.
That’s also why we’re here today. We think giving back to the community is central to our shared success. We will use this page to share product updates, interesting use-cases, research & development progress, and much more. We want this to be a place for conversation and debate, one that ultimately is for the benefit of all.
So if you want to follow and participate in the Motherbrain journey, click the follow button below. Our comments section is always open. We look forward to speaking to you again soon!
</p>
</div>
<div class="article">
<h2>Using Deep Learning to Find the Next Unicorn: A Practical Synthesis</h2>
<p>Oct 31, 2022
Save
Machine Learning is increasingly being succeeded by Deep Learning as the method of choice for investors looking to predict startup success. But academic research into Deep Learning has been limited to date. EQT Motherbrain is changing that.
On the hunt for a unicorn
The life of a startup is a treacherous one. When done well, they are a source of innovation, disruption and highly scalable opportunity. Many of today’s most important economic and social developments can be sourced back to a startup.
But these positive aspects are almost always hamstrung from day one. Limited funding and a lack of human resources are just two of the major challenges that face-up from the moment of inception. Making it through the early stages is rare, which makes it equally difficult to identify winners. It’s like “spotting a unicorn in the wild’’.
But for venture capitalists, this is the name of the game. They strive to identify and invest in unicorn startups during their early stages, hoping to gain a high return. How can Deep Learning help?
This is what EQT Motherbrain has attempted to answer. In conjunction with co-authors from Stockholm School of Economics and Stockholm University, we have recently led a research effort to review published literature that address the problem of startup success prediction using Deep Learning methods. We’re very excited to be sharing this publicly in our latest working paper.
From Machine Learning to Deep Learning
To avoid entirely relying on human domain expertise and intuition, investors usually employ data-driven approaches to forecast the success probability of startups. Over the past two decades, the industry has gone through a paradigm shift moving from conventional statistical approaches towards becoming machine-learning (“ML”) oriented.
Notably, the recent rapid growth of data volume and variety is quickly ushering in deep learning (“DL”), a subset of ML, as a potentially superior approach in terms capacity (how much data a model can fit) and expressivity (how many types of data the model can approximate). However, there has not yet been a comprehensive synthesis of the existing DL based research. This leaves many practitioners uninformed and vulnerable to some pitfalls hidden in nine key tasks: problem scoping, success definition, data gathering, data processing, data split, model selection, model evaluation, model explanation, and model productization.
To overcome these challenges, EQT Motherbrain has carried out a literature synthesis on DL based methods, reviewing the material through a practical lens. The key statistics and learnings are presented from nine perspectives (corresponding to the aforementioned nine tasks) in the form of a working paper. By reading this paper, a practitioner in the field may be able to obtain a more thorough and in-depth understanding of the methodologies around startup evaluation with DL, and distill valuable and actionable learnings.
We would love to hear your thoughts about the paper. If you have any questions, please email the authors Lele Cao (lele.cao@eqtpartners.com) or Vilhelm von Ehrenheim (vilhelm.vonehrenheim@eqtpartners.com).
</p>
</div>
<div class="article">
<h2>Predicting Revenue for Scaleup Companies</h2>
<p>Nov 23, 2022
Save
EQT Motherbrain is excited to share a novel approach it has developed for predicting the revenue of scaleup companies. It was presented recently at the 31st ACM International Conference on Information and Knowledge Management (CIKM 2022) by Lele Cao, Sonja Horn, Vilhelm von Ehrenheim, Richard Anselmo Stahl, and Henrik Landgren.
The underlying problem
Predicting a company’s financial future is a complex yet necessary task, one that is critical for businesses’ decision-making. Fundamentally, it is an informed guess that relies on examining historical performance data. Yet, as we have all seen, due to random elements that cannot be incorporated into a model, forecasts can easily break down.
Investors rely on financial forecasts when determining the valuation of a business. Whether relying on discounted future cash flows or determining the valuation on a multiple basis of future EBITDA, forecasts can make or break an investment. For mature businesses with a long track record and resilient cash flows, deviations between predictions and the actual outcomes might not be significant. But for businesses with insufficient historical data and inherently uncertain future prospects, such as startups and scaleups, this looks different.
A start-up moves into scaleup territory when demonstrating the scalability and viability of its business model and experiencing an accelerated cycle of revenue growth. The raising of outside capital usually accompanies this transition.
In comparison to most mature companies, scaleups are often not profitable. Consequently, revenue becomes one of the most important metrics when evaluating scaleups, and valuation is typically determined on a multiple basis of future revenues from which investment professionals extrapolate future company revenues.
Revenue forecasting is typically done manually and empirically, leaving the quality heavily dependent on the investment professionals’ experiences. Factors such as business model, competitor landscape, market trends, and unit economics are considered. The task is essential to evaluating the attractiveness of an investment, as it informs the change in valuation during the ownership period. However, the level of automation, objectiveness, consistency, and adaptability of this approach is far from optimal.
Quantitative methods such as traditional statistical approaches or newly developed AI-based methodologies have been increasingly adopted in the forecasting of traditional and mature businesses. But why hasn’t this spread over to the world of start-ups and scaleups?
The answer lies in the data, which for immature businesses is often proprietary and costly to acquire. However, this is changing — the ubiquity of digitization means that large amounts of high-quality data on private companies are progressively more accessible publicly.
Introducing SiRE
We call the answer to this challenge SiRE, a simulation-informed revenue extrapolation model.
SiRE is based on a Kalman Filter, a methodology typically used for the navigation and control of aircrafts and spaceships. It is sector-agnostic, permitting investors to apply it across multiple industries. It only needs small datasets of a few hundred scaleup companies for training, and the extrapolation can commence from short revenue time series enabling revenue predictions even without granular historical data. It can produce fine-grained predictions of multiple years, accommodating the typical investment periods of 5 years and longer. Each revenue prediction comes with a confidence estimate, providing investors with guidance on outcome certainty. The model is easy to implement, and predictions are explainable, promoting transparency to build trust and capture feedback.
SiRE is designed on the main assumption that revenue development is likely to repeat historical patterns for similar companies at a similar stage. Each future revenue point is initially obtained by sampling from the comparable revenue states, then adjusted with a Kalman Filter that considers historical and predicted revenue points. The prediction confidence is estimated by extrapolating multiple times.
Demonstrating how we use SiRE within EQT
So, how do we use it within EQT? Trained on a proprietary dataset of revenue trajectories from our portfolio companies and other data we have collected through nearly three decades of investing, SiRe can be used in two ways:
For example, in August 2021, we received data from a company that had approximately USD 10 million in revenue, growing 150% year-over-year. Our revenue prediction model predicted with 95% confidence that the company would end up with revenue between USD 29 million and USD 40 million 12 months later. When receiving the actual data, we were delighted to find out that the company ended up with USD 30 million.
To learn more about SiRE and how it works, find an explanation of the method in our paper and the source code here
</p>
</div>
<div class="article">
<h2>Disrupting private capital using machine learning and an event-driven architecture</h2>
<p>Dec 14, 2022
Save
Written by Ylva Lundegård and Dhiana Deva
There are currently hundreds of millions of companies worldwide, and thousands of new companies are founded daily. It is extremely challenging to track and analyze them all, but EQT has a secret weapon, Motherbrain. Let’s take a look at the data architecture behind the Motherbrain platform that uses big data and machine learning to support a variety of applications and use cases in the investment industry. Motherbrain’s data pipeline is built with scalability and robustness in mind to withstand large volumes of noisy data.
External data sources
External data is ingested through batch pipelines that clean, normalize and store the data in a uniform way. All data pipelines are written in Apache Beam, and run on Dataflow.
Data logs
To make the architecture fit with all types of data formats, the data is reduced to small pieces of information on an attribute level, together with metadata. It is then stored in BigTable, while a Kafka message is sent to notify there is new or updated data. This abstraction makes it possible to keep the data loosely coupled and easily merge data entities into a single entity downstream. This also enables flexibility when changing either ingestion jobs or entity structures downstream.
Materializers
Next up is what the EQT Motherbrain calls materializers, the streaming pipelines responsible for putting together the entities again after listening to the Kafka messages. Two things are essential for the materializers to decide here: (1) what should go into the entity and (2) what is the most likely correct information. There can be contradictory information from different sources, e.g. one data source saying that a company is named “Vandelay Industries” while another is saying “Vandelay Ind.” This is where Motherbrain needs to make a decision about which one to pick.
A core part of putting the entities together is to have a good entity resolution, which is where the entity matching service comes into play. It’s an online prediction service implemented using Scala, Vertex AI, Tensorflow, and Elasticsearch. This service is responsible for deciding what data from the different sources belong together.
Consumers
After entities have been built, they are sent to Kafka to be consumed by other applications. The primary consumer of the data is Motherbrain’s web UI, which is made easily accessible via Elasticsearch. Other consumers are the superusers, mainly the Motherbrain Labs team, who access the data from BigQuery and usually do more ad-hoc analysis.
User input
Apart from the external data, EQT users continuously provide valuable proprietary data via the Motherbrain platform. This goes through a similar flow as the external data sources but is generally prioritized higher by Motherbrain when materializing the entities or training models.
The architecture of Motherbrain needs to be dynamic to react to the ever-changing world. EQT Motherbrain’s solution to this has been to build an event-driven architecture that can break down data into smaller bits and pieces whenever needed, add intelligence to it, and put it back together to empower EQT to make faster, smarter, and more conscious decisions.
This post is based on our Heroes of Data talk summary
</p>
</div>
<div class="article">
<h2>Applying Transformers to Score Potentially Successful Startups</h2>
<p>Feb 6
Save
As investment firms aim to invest in promising companies, they constantly face the critical task of estimating the chance of success for these. In Venture Capital, this task is often referred to as Startup Success Prediction (SSP).
“Finding the most promising, likely-to-succeed startups for early-stage investment is the holy grail of data-driven Venture Capital. We work with this continuously at EQT Ventures and Motherbrain, but using models for it is difficult: startup data is sparse and noisy, and the rare successes can only be proven years after the initial funding rounds. Additionally, the definition of success changes over time. For a prediction model to be resistant, it needs to identify the constant, underlying signals that stand the test of time and universally define successful early-stage companies.”
– Anton Ask Åström, Analytics Lead, EQT Ventures
Traditionally, Venture Capital firms have mostly relied on analytical, statistical, or manual methods for sourcing new startup deals, but recently Deep Learning (DL) has been increasingly utilized for this task. One reason is the complexity of the task requires making predictions based on incomplete information — mostly due to startups lacking a lot of public information in their early stages.
Additionally, the correlations and signals indicating success may exist in dramatically different configurations, depending on the startup in focus. Therefore, a DL-based method needs to be able to successfully model complex correlations amongst the input features and often correlations between numerous different input features to compensate for the data shortage. These DL-based methods are also able to sift through magnitudes more data compared to their non-DL alternatives.
Adding more complexity to the SSP problem is the importance of time. A performant method should be phase-agnostic meaning it finds a potentially successful startup regardless of which phase the startup is in at the time of evaluation. Therefore, even if the successful startup has just been founded, or is just in the process of entering a growth phase, it should be correctly identified.
To summarize, SSP as a DL problem has the following three challenges:
Capturing the temporal correlations between different features is important for the SSP problem. Therefore, many existing methods utilize specifically time series — a sequence of values. With time series data, the prediction can be potentially guided by the time, order, and frequency of these sequential values, which is an important characteristic of SSP.
In the past, statistical methods have been surprisingly dominant within the field of Time Series Classification, and it is only recently that DL-based methods such as the Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU) have been catching up in terms of performance. This can be observed specifically within the context of multivariate time series classification, where the complexity of the problem increases. Unlike univariate time series classification, the model needs to capture patterns across multiple time series, and in return hopefully yield improved performance. A univariate structure could also be extended to include multiple time series features separately, a common solution if the time series features that are available do not have the same frequency.
Another consideration for SSP is the choice of evaluation strategy. We previously mentioned the challenge of correctly identifying startups regardless of the phase in their lifecycle. To address this, one can opt for an investor-centric data split strategy — replacing the traditional random sampling strategy. Concretely, we extrapolate a single sample into multiple samples (each representing the company at different stages in time), and build the evaluation dataset on only the most recent samples. As a result, the model is expected to act phase-agnostically while it also better resembles the real-world usage of the model by investment professionals.
In a paper from 2021, the authors presented a Transformer-based method and architecture, called the TST, for multivariate time series classification. The architecture showed promising results on public time series datasets, even outperforming state-of-the-art statistical and DL-based methods. However, the TST had yet to be adapted and tested on real-world data with all the accompanying challenges, specifically within the investment domain.
This is where we at Motherbrain saw the potential to improve our existing scoring models. By focusing solely on time series features from a variety of data sources — and the labels from our in-house investment professionals — the TST architecture was modified and implemented to potentially further improve our scoring models.
When trained and evaluated on the labeled EQT dataset, several interesting results emerged. Firstly, a Univariate GRU model was implemented where each time series feature was given a separate GRU block. Additionally, a Multivariate GRU was implemented, where all features share one common GRU block. This was done to help determine if any improvements observed were due to the Transformer, or just the introduction of multivariate learning itself.
In our testing, the TST model showed a 12% increase in test performance (from 0.81 to 0.92 AUC) compared to the Multivariate GRU and as much as a 47% increase (0.62 AUC) compared to the Univariate GRU. The model was also tested on two public datasets from the public UCR archive (Ethanol and PEMS-SF datasets). The purpose was to gain even more insights into how the model performs for different datasets. The TST once again showed the highest average accuracy across these datasets.
Training TimesAdditionally, the cost of increased training times for TST was limited. While the GRU is the simplest architecture computationally, only a 4x increase in training times was observed for TST compared to the Multivariate GRU. Furthermore, for the Univariate GRU, the TST even decreased its training times by over 15x. Furthermore, the multi-head attention mechanism of the Transformer allows the network to parallelize its computations on modern GPUs to decrease its training times and therefore makes it possible to train models on dramatically larger datasets.
Training StabilityWhile the Transformer is known to be potentially difficult and unstable to train, we did not encounter these issues. In fact, the TST was the most stable out of all four models, delivering the lowest standard deviation amongst different training runs.
Besides achieving higher prediction performance, the Transformer can bring other potential advantages to the table as well.
Better explainabilityThe attention mechanism inherently produces a weighted matrix of how its weights are distributed along the input sequence. Therefore, one can gain insights, on an instance level, into what parts of the sequence the model focuses on, and in turn, achieve a higher explainability of the model as a whole.
Embeddings utilizationThrough its use of the Encoder-Decoder architecture, it inherently uses embeddings and is therefore more effectively integrated with, for example, a language model to achieve a multimodal architecture. Additionally, the embeddings learned by the model can be used for other downstream tasks.
Unsupervised learning capabilityA large contribution of the original TST paper was the introduction of an unsupervised masked learning routine. By masking parts of the time series sequences, and training the network to predict the masked part, the model will theoretically learn a more effective representation before being fine-tuned on labeled data. While not thoroughly tested yet by us, this approach could drive the most potential, since it would allow the model to train on the considerably more abundant and cheaper unlabeled data.
By its nature, SSP is a complex problem that requires sifting through massive, but sparse, amounts of data to find the startups in the world that show real potential. We at EQT have long seen the potential in aiding this through our data-driven platform Motherbrain and its scoring models.
The TST we explored shows the promise of Transformer-based models for SSP in the future where the results suggest it to be a performant alternative to the GRU architecture for multivariate time series classification in the investment domain. The architecture can also easily be generalized to work in other investment contexts such as the Growth and Private Equity funds and therefore provide some exciting future possibilities.
</p>
</div>
<div class="article">
<h2>The voyage towards GPT-Enabled M&A</h2>
<p>Mar 20
Save
Proofreaders: Lele Cao, Alex Carusillio & Chat GPT
EQT launched Motherbrain in 2016, primarily focused on supporting the Venture fund in finding the next tech unicorn. Today, Motherbrain is used across the business, powering everything from early-stage investment all the way through to multi-billion dollar buyouts.
One of Motherbrain’s key services is supporting add-on acquisitions, which is where EQT supports its portfolio companies in acquiring a smaller company, usually one that offers complementary products or services. In recent months we have introduced Generative Pre-trained Transformer (GPT) models, similar to Chat GPT, to speed up the M&A sourcing process by enabling our portfolio to scan hundreds of thousands of companies. In this article, we will look back at the evolution of Motherbrain M&A sourcing and share some learnings from the journey.
Motherbrain’s first endeavor to support M&A sourcing was by introducing a product called Spaces, in which investment professionals began by submitting a description of a target company.
The goal with Spaces was to train an artificial intelligence. It functioned much like the popular dating app Tinder. Companies would appear and investment professionals would swipe left or right depending on the company interests. Behind the scenes, the platform used machine learning to identify similar companies based on the user’s description and feedback. Once the AI had collected enough swipes, it produced a final list of potential acquisition targets for the human to review. The final list is the AI’s interpretation of what is relevant based on the reinforcement it received. It could pick up any underlying trends in the text, like similar technologies, phrases, or other commonalities, enabling it to find new angles of connecting the dots between companies that are less natural for a human to derive.
While this approach may sound intelligent, the team later realized it was a prime example of engineers shoehorning technology into a product without fully considering the end user’s needs. Investment professionals need control and the ability to audit the data before trusting a solution.
We encountered problems with Spaces when companies that the investment professionals expected to be in the list did not appear, which led us to losing trust.
Our solution needed more explainability, enabling the user to assess the underlying data and build conviction. Secondly, our users requested more control. Investment professionals often have specific valuation, regional, or revenue requirements that make something “the right target”. These factors were not part of the user input to the Spaces AI.
Following the creation of Spaces, we learned we needed to invest in a team that could work closely with specific deals to learn and adapt our approach while providing more value per case, shifting from autonomy to bespoke analysis. This became Motherbrain Labs, a team of seasoned technologists that collaborate with investment professionals and EQT’s digital arm to develop software-based solutions that address nuanced business challenges, which conventional means cannot address. Identifying potential acquisition targets has become a particular area of competitive advantage for the team.
Labs evolved from using Spaces to a more practical approach called List Expansion. List Expansion is a versatile solution that works by analyzing a set of interesting companies and their key characteristics. You select the criteria that are most important to you and it will identify similar companies from our extensive datasets.
The figure above illustrates this process in action. From the left it shows the data sources we used to find similar companies. The user then controls and filters out companies based on their specific requirements, such as region, FTEs, valuation, and more (seen as the Sanky flows to the right in the figure). All filtered out companies are stored separately, so you can inspect them later for traceability and control. Finally, the orange line in the figure shows the list of companies that fulfill all your criteria and require manual review. Like Space’s final list, the orange line presents the relevant company’s firmographics but now fully reflects the user’s input and is no longer produced by an AI.
This workflow allowed investment professionals to understand where companies got filtered out and how many they have left as relevant targets, then they could backtest the results for quality control. It gained the trust we sought, but the iteration speed was still slow.
Two main factors hindered the iteration process. First, it required an extensive manual review of companies, some of which were only partially aligned with the desired businesses. Second, after the manual examination, the list was discussed with the portfolio company, where differing perspectives on specific needs within the field often necessitated further rounds of iteration to arrive at a satisfactory list.
The release and commoditization of OpenAI’s Chat GPT has brought the power of machine learning to a wider audience. One particularly intriguing aspect is the ability to create a customized GPT-3 model on top of the OpenAI engine, which can be tailored to excel in a specific domain — in our case, M&A.
In ​​January 2023, we deployed our new solution using the OpenAI engine. It allows users to describe a variety of businesses of interest, then we extract all companies matching the criteria and let a bespoke GPT model analyze the dataset. The user can manually review the extensive collection of companies or use our GPT model to speed up the process by asking the model questions about the business in the set. Similarly to Chat GPT, you can ask questions like “provide me companies using computer vision and machine learning for tracking X in Y industry.” The bot will answer by providing companies that match the query. As the model is tuned to understand the provided set of companies rather than the whole web corpus, it has enabled a more specialized performance resulting in a leap ahead of Chat GTP. This approach furthermore solves the previous difficulties of backtesting, user control and simultaneously gains the efficiency of AI.
So far, this solution is live for one of our portfolio companies, where it has been highly effective. They have found and reached out to several new targets and are using our software platform as their primary way of working with M&A. Our meetings have shifted from understanding the necessities of the add-ons to discussing the businesses they have found and the actionability.
While we were successful in creating a valuable solution for one of our portfolio companies using GPT models, we understand that not every situation can be approached in the same way. While some may focus on the potential of this technology for M&A sourcing, our intention with this text is to showcase the value of engineering working closely with investment professionals and portfolio companies. This close collaboration enables one to build an infrastructure that is flexible, adaptive and can quickly respond to the changing needs of the business; in this particular case, using OpenAI as part of the deliverable.
</p>
</div>

    </body>
    </html>
    
